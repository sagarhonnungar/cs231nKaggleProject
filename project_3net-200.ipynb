{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import keras as k\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score, precision_score \n",
    "from skimage import io,transform\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    fill_mode='reflect',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "def multilabelmetrics(y_true,y_pred):\n",
    "    '''y_true and y_pred should be boolean np arrays\n",
    "    of shape num_example x num_classes '''\n",
    "    total = np.sum(y_true,axis = 0)\n",
    "    tp = np.sum(y_true*y_pred,axis=0)\n",
    "    tn = np.sum((1-y_true)*(1-y_pred),axis=0)\n",
    "    fp = np.sum((1-y_true)*y_pred,axis=0)\n",
    "    fn = np.sum(y_true*(1-y_pred),axis=0)\n",
    "    return total,tp,tn,fp,fn\n",
    "\n",
    "def combine_predictions(x,y,y1,y2,thresh,thresh1,thresh2,thresh3):\n",
    "    y_pred = np.zeros((x.shape[0],17),np.uint8)\n",
    "    y_bool = np.array((y > thresh),np.uint8)\n",
    "    y1_bool = np.array((y1 > thresh1),np.uint8)\n",
    "    y2_bool = np.array((y2 > thresh2)*np.tile(y1[:,0]>thresh3,(7,1)).T,np.uint8)\n",
    "    y_pred[:,:7] = y2_bool\n",
    "    y_pred[:,7:13] = y1_bool[:,1:]\n",
    "    y_pred[:,13:] = y_bool\n",
    "    return y_pred\n",
    "\n",
    "def combine_predictions_2(x,y,y1,y2,thresh,thresh1,thresh2,thresh3,thresh4):#if cloud > thresh4, \n",
    "    y_pred = np.zeros((x.shape[0],17),np.uint8)\n",
    "    y_bool = np.array((y > thresh),np.uint8)\n",
    "    y1_bool = np.array((y1 > thresh1),np.uint8)\n",
    "    y2_bool = np.array((y2 > thresh2)*np.tile(y1[:,0]>thresh3,(7,1)).T,np.uint8)\n",
    "    y_pred[:,:7] = y2_bool\n",
    "    y_pred[:,7:13] = y1_bool[:,1:]\n",
    "    y_pred[:,13:] = y_bool\n",
    "    y_pred[((np.nonzero(y[:,1]>thresh4))[0]),:] = 0\n",
    "    y_pred[((np.nonzero(y[:,1]>thresh4))[0]),14] = 1\n",
    "    return y_pred\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2, verbose=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36000/36000 [04:20<00:00, 138.29it/s]\n",
      "100%|██████████| 4479/4479 [00:21<00:00, 212.20it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train_v2.csv')\n",
    "\n",
    "labels = ['blow_down',\n",
    " 'bare_ground',\n",
    " 'conventional_mine',\n",
    " 'blooming',\n",
    " 'artisinal_mine',\n",
    " 'selective_logging',         \n",
    " 'slash_burn', \n",
    " 'cultivation',\n",
    " 'habitation',\n",
    " 'road',\n",
    " 'agriculture',\n",
    " 'water',\n",
    " 'primary',\n",
    " 'partly_cloudy', \n",
    " 'cloudy',\n",
    " 'clear',\n",
    " 'haze',]\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "np.random.shuffle(df_train.values)\n",
    "\n",
    "train_values = df_train.values[:36000]\n",
    "val_values = df_train.values[36000:]\n",
    "\n",
    "x_train = np.zeros((36000,200,200,3), np.float32)\n",
    "x_val = np.zeros((40479-36000,200,200,3), np.float32)\n",
    "y_train = []\n",
    "y_val = []\n",
    "\n",
    "i=0\n",
    "\n",
    "for f, tags in tqdm(train_values, miniters=1000):    \n",
    "    img = cv2.imread('train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x_train[i,:,:,:] = np.array(cv2.resize(img, (200, 200)),np.float32)/255.#139 minimum size for inception\n",
    "    i+=1\n",
    "    y_train.append(targets)\n",
    "\n",
    "i=0\n",
    "\n",
    "for f, tags in tqdm(val_values, miniters=1000):    \n",
    "    img = cv2.imread('train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x_val[i,:,:,:] = np.array(cv2.resize(img, (200, 200)),np.float32)/255.#139 minimum size for inception\n",
    "    i+=1\n",
    "    y_val.append(targets)\n",
    "  \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "y_val = np.array(y_val, np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#subtracting mean\n",
    "train_mean = np.mean(x_train,axis = 0)\n",
    "x_train -= train_mean\n",
    "x_val -= train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 200, 200, 3)\n",
      "(36000, 4)\n",
      "(4479, 200, 200, 3)\n",
      "(4479, 4)\n"
     ]
    }
   ],
   "source": [
    "#weather classifier (last four labels - mutually exclusive)\n",
    "#x_train, x_val, y_train_w, y_val_w = train_test_split(x_train,y_train[:,-4:],test_size=0.1)\n",
    "y_train_w = y_train[:,-4:]\n",
    "y_val_w = y_val[:,-4:]\n",
    "print(x_train.shape)\n",
    "print(y_train_w.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()#using same architecture for all three models\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', input_shape=(200, 200, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1024))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/562 [==============================] - 705s - loss: 0.4580 - acc: 0.8435 - val_loss: 0.3631 - val_acc: 0.8598\n",
      "Epoch 2/10\n",
      "563/562 [==============================] - 703s - loss: 0.3364 - acc: 0.8829 - val_loss: 0.7531 - val_acc: 0.7459\n",
      "Epoch 3/10\n",
      "563/562 [==============================] - 704s - loss: 0.3159 - acc: 0.8867 - val_loss: 0.2698 - val_acc: 0.8951\n",
      "Epoch 4/10\n",
      "563/562 [==============================] - 704s - loss: 0.2914 - acc: 0.8968 - val_loss: 0.2871 - val_acc: 0.8993\n",
      "Epoch 5/10\n",
      "563/562 [==============================] - 704s - loss: 0.2882 - acc: 0.9002 - val_loss: 0.4651 - val_acc: 0.8651\n",
      "Epoch 6/10\n",
      "563/562 [==============================] - 704s - loss: 0.2817 - acc: 0.9013 - val_loss: 0.5382 - val_acc: 0.8542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f568cd98518>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])    \n",
    "model.fit_generator(datagen.flow(x_train,y_train_w, batch_size = 64), validation_data=(x_val, y_val_w),\n",
    "                  epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.89351364569\n",
      "thresh: 0.1 \tF2 score: 0.901145823366\n",
      "thresh: 0.15 \tF2 score: 0.899711882967\n",
      "thresh: 0.2 \tF2 score: 0.895964235214\n",
      "thresh: 0.25 \tF2 score: 0.8921049554\n",
      "thresh: 0.3 \tF2 score: 0.883232864479\n",
      "thresh: 0.35 \tF2 score: 0.875009302672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_w, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/562 [==============================] - 706s - loss: 0.2483 - acc: 0.9104 - val_loss: 0.2089 - val_acc: 0.9225\n",
      "Epoch 2/10\n",
      "563/562 [==============================] - 705s - loss: 0.2392 - acc: 0.9146 - val_loss: 0.2287 - val_acc: 0.9181\n",
      "Epoch 3/10\n",
      "563/562 [==============================] - 706s - loss: 0.2362 - acc: 0.9144 - val_loss: 0.2584 - val_acc: 0.9011\n",
      "Epoch 4/10\n",
      "563/562 [==============================] - 706s - loss: 0.2272 - acc: 0.9185 - val_loss: 0.2289 - val_acc: 0.9181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5672edabe0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continue with reduced learning rate\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.0005),\n",
    "              metrics=['accuracy']) \n",
    "model.fit_generator(datagen.flow(x_train,y_train_w, batch_size = 64), validation_data=(x_val, y_val_w),\n",
    "                  epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.931158900265\n",
      "thresh: 0.1 \tF2 score: 0.941688195707\n",
      "thresh: 0.15 \tF2 score: 0.944084298153\n",
      "thresh: 0.2 \tF2 score: 0.942174592543\n",
      "thresh: 0.25 \tF2 score: 0.940207741949\n",
      "thresh: 0.3 \tF2 score: 0.936592989507\n",
      "thresh: 0.35 \tF2 score: 0.932313760512\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_w, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/562 [==============================] - 708s - loss: 0.2095 - acc: 0.9222 - val_loss: 0.1896 - val_acc: 0.9283\n",
      "Epoch 2/10\n",
      "563/562 [==============================] - 706s - loss: 0.2059 - acc: 0.9236 - val_loss: 0.1922 - val_acc: 0.9259\n",
      "Epoch 3/10\n",
      "563/562 [==============================] - 706s - loss: 0.2005 - acc: 0.9264 - val_loss: 0.1894 - val_acc: 0.9283\n",
      "Epoch 4/10\n",
      "563/562 [==============================] - 706s - loss: 0.2042 - acc: 0.9250 - val_loss: 0.1988 - val_acc: 0.9203\n",
      "Epoch 5/10\n",
      "563/562 [==============================] - 706s - loss: 0.2014 - acc: 0.9260 - val_loss: 0.1774 - val_acc: 0.9299\n",
      "Epoch 6/10\n",
      "563/562 [==============================] - 706s - loss: 0.2017 - acc: 0.9257 - val_loss: 0.1883 - val_acc: 0.9312\n",
      "Epoch 7/10\n",
      "563/562 [==============================] - 706s - loss: 0.1968 - acc: 0.9272 - val_loss: 0.2043 - val_acc: 0.9254\n",
      "Epoch 8/10\n",
      "563/562 [==============================] - 706s - loss: 0.1959 - acc: 0.9279 - val_loss: 0.2417 - val_acc: 0.9087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5671377860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continue with reduced learning rate\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(lr=0.0002),\n",
    "              metrics=['accuracy']) \n",
    "model.fit_generator(datagen.flow(x_train,y_train_w, batch_size = 64), validation_data=(x_val, y_val_w),\n",
    "                  epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.923089497018\n",
      "thresh: 0.1 \tF2 score: 0.932467919072\n",
      "thresh: 0.15 \tF2 score: 0.934344400855\n",
      "thresh: 0.2 \tF2 score: 0.932919763127\n",
      "thresh: 0.25 \tF2 score: 0.930155540671\n",
      "thresh: 0.3 \tF2 score: 0.927598634899\n",
      "thresh: 0.35 \tF2 score: 0.921597082682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_w, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/562 [==============================] - 708s - loss: 0.1911 - acc: 0.9299 - val_loss: 0.1740 - val_acc: 0.9306\n",
      "Epoch 2/10\n",
      "563/562 [==============================] - 705s - loss: 0.1869 - acc: 0.9310 - val_loss: 0.1758 - val_acc: 0.9315\n",
      "Epoch 3/10\n",
      "563/562 [==============================] - 706s - loss: 0.1899 - acc: 0.9297 - val_loss: 0.1725 - val_acc: 0.9348\n",
      "Epoch 4/10\n",
      "563/562 [==============================] - 706s - loss: 0.1854 - acc: 0.9315 - val_loss: 0.1823 - val_acc: 0.9308\n",
      "Epoch 5/10\n",
      "563/562 [==============================] - 706s - loss: 0.1867 - acc: 0.9309 - val_loss: 0.1726 - val_acc: 0.9332\n",
      "Epoch 6/10\n",
      "563/562 [==============================] - 706s - loss: 0.1832 - acc: 0.9324 - val_loss: 0.1730 - val_acc: 0.9339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f564af1c470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continue with reduced learning rate\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train,y_train_w, batch_size = 64), validation_data=(x_val, y_val_w),\n",
    "                  epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.942299514135\n",
      "thresh: 0.1 \tF2 score: 0.951243368524\n",
      "thresh: 0.15 \tF2 score: 0.953550431112\n",
      "thresh: 0.2 \tF2 score: 0.955193017149\n",
      "thresh: 0.25 \tF2 score: 0.953305903741\n",
      "thresh: 0.3 \tF2 score: 0.951599527956\n",
      "thresh: 0.35 \tF2 score: 0.948016670388\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_w, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/562 [==============================] - 709s - loss: 0.1812 - acc: 0.9323 - val_loss: 0.1735 - val_acc: 0.9337\n",
      "Epoch 2/10\n",
      "563/562 [==============================] - 706s - loss: 0.1811 - acc: 0.9336 - val_loss: 0.1716 - val_acc: 0.9344\n",
      "Epoch 3/10\n",
      "563/562 [==============================] - 706s - loss: 0.1804 - acc: 0.9330 - val_loss: 0.1737 - val_acc: 0.9330\n",
      "Epoch 4/10\n",
      "563/562 [==============================] - 706s - loss: 0.1787 - acc: 0.9345 - val_loss: 0.1720 - val_acc: 0.9339\n",
      "Epoch 5/10\n",
      "563/562 [==============================] - 706s - loss: 0.1790 - acc: 0.9333 - val_loss: 0.1744 - val_acc: 0.9326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56493b4630>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continue with reduced learning rate\n",
    "model.compile(optimizer=Adam(lr=0.00005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train,y_train_w, batch_size = 64), validation_data=(x_val, y_val_w),\n",
    "                  epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.944508234193\n",
      "thresh: 0.1 \tF2 score: 0.953499930894\n",
      "thresh: 0.15 \tF2 score: 0.954725225656\n",
      "thresh: 0.2 \tF2 score: 0.954066064917\n",
      "thresh: 0.25 \tF2 score: 0.952668006251\n",
      "thresh: 0.3 \tF2 score: 0.948511040942\n",
      "thresh: 0.35 \tF2 score: 0.944928183374\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_w, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(\"simple_200_weather\")\n",
    "model.save_weights(\"simple_200_weather_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36000/36000 [05:10<00:00, 115.93it/s]\n",
      "100%|██████████| 4479/4479 [00:44<00:00, 100.87it/s]\n"
     ]
    }
   ],
   "source": [
    "x_val = []\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "df_train = pd.read_csv('train_v2.csv')\n",
    "\n",
    "labels = ['blow_down',\n",
    " 'bare_ground',\n",
    " 'conventional_mine',\n",
    " 'blooming',\n",
    " 'artisinal_mine',\n",
    " 'selective_logging',         \n",
    " 'slash_burn', \n",
    " 'cultivation',\n",
    " 'habitation',\n",
    " 'road',\n",
    " 'agriculture',\n",
    " 'water',\n",
    " 'primary',\n",
    " 'partly_cloudy', \n",
    " 'cloudy',\n",
    " 'clear',\n",
    " 'haze',]\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "np.random.shuffle(df_train.values)\n",
    "\n",
    "train_values = df_train.values[:36000]\n",
    "val_values = df_train.values[36000:]\n",
    "\n",
    "x_train = np.zeros((36000,200,200,3), np.float32)\n",
    "x_val = np.zeros((40479-36000,200,200,3), np.float32)\n",
    "y_train = []\n",
    "y_val = []\n",
    "\n",
    "i=0\n",
    "\n",
    "for f, tags in tqdm(train_values, miniters=1000):    \n",
    "    img = cv2.imread('train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x_train[i,:,:,:] = np.array(cv2.resize(img, (200, 200)),np.float32)/255.#139 minimum size for inception\n",
    "    i+=1\n",
    "    y_train.append(targets)\n",
    "\n",
    "i=0\n",
    "\n",
    "for f, tags in tqdm(val_values, miniters=1000):    \n",
    "    img = cv2.imread('train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x_val[i,:,:,:] = np.array(cv2.resize(img, (200, 200)),np.float32)/255.#139 minimum size for inception\n",
    "    i+=1\n",
    "    y_val.append(targets)\n",
    "  \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "y_val = np.array(y_val, np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_2 = np.zeros((y_train.shape[0],7))\n",
    "y_train_2[:,1:] = y_train[:,7:13]\n",
    "y_train_2[:,0] = (np.sum(y_train[:,:7],axis=1)>0)\n",
    "y_train_2 = np.array(y_train_2,np.uint8)\n",
    "\n",
    "y_val_2 = np.zeros((y_val.shape[0],7))\n",
    "y_val_2[:,1:] = y_val[:,7:13]\n",
    "y_val_2[:,0] = (np.sum(y_val[:,:7],axis=1)>0)\n",
    "y_val_2 = np.array(y_val_2,np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0]\n",
      "[0 0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[100,:])\n",
    "print(y_train_2[100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36000, 200, 200, 3)\n",
      "(36000, 7)\n",
      "(4479, 200, 200, 3)\n",
      "(4479, 7)\n"
     ]
    }
   ],
   "source": [
    "train_mean = np.load('simple_200_train_mean.npy')\n",
    "x_train -= train_mean\n",
    "x_val -= train_mean\n",
    "#x_train, x_val, y_train_2, y_val_2 = train_test_split(x_train,y_train_2,test_size=0.1)\n",
    "print(x_train.shape)\n",
    "print(y_train_2.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model for the more common classes + 1 class for others\n",
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, (3, 3), padding = 'same', input_shape=(200,200,3)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(2048))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(1024))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(7, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/562 [==============================] - 745s - loss: 0.2885 - acc: 0.8823 - val_loss: 0.2816 - val_acc: 0.8858\n",
      "Epoch 2/10\n",
      "563/562 [==============================] - 732s - loss: 0.2423 - acc: 0.9017 - val_loss: 0.3013 - val_acc: 0.8828\n",
      "Epoch 4/10\n",
      "563/562 [==============================] - 732s - loss: 0.2284 - acc: 0.9084 - val_loss: 0.2346 - val_acc: 0.9047\n",
      "Epoch 5/10\n",
      "563/562 [==============================] - 733s - loss: 0.2161 - acc: 0.9130 - val_loss: 0.2199 - val_acc: 0.9105\n",
      "Epoch 6/10\n",
      "563/562 [==============================] - 733s - loss: 0.2090 - acc: 0.9168 - val_loss: 0.2006 - val_acc: 0.9178\n",
      "Epoch 7/10\n",
      "563/562 [==============================] - 734s - loss: 0.2027 - acc: 0.9195 - val_loss: 0.2327 - val_acc: 0.8990\n",
      "Epoch 8/10\n",
      "563/562 [==============================] - 734s - loss: 0.1981 - acc: 0.9217 - val_loss: 0.2090 - val_acc: 0.9150\n",
      "Epoch 9/10\n",
      "563/562 [==============================] - 734s - loss: 0.1946 - acc: 0.9228 - val_loss: 0.2672 - val_acc: 0.8788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ee16d2898>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.fit_generator(datagen.flow(x_train,y_train_2, batch_size = 64), validation_data=(x_val, y_val_2),\n",
    "                   epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.82424984073\n",
      "thresh: 0.1 \tF2 score: 0.837270896841\n",
      "thresh: 0.15 \tF2 score: 0.833254790366\n",
      "thresh: 0.2 \tF2 score: 0.825835942826\n",
      "thresh: 0.25 \tF2 score: 0.815065760238\n",
      "thresh: 0.3 \tF2 score: 0.802570581189\n",
      "thresh: 0.35 \tF2 score: 0.790765886788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model1.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_2, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/562 [==============================] - 735s - loss: 0.1855 - acc: 0.9261 - val_loss: 0.1826 - val_acc: 0.9283\n",
      "Epoch 2/10\n",
      "563/562 [==============================] - 734s - loss: 0.1822 - acc: 0.9279 - val_loss: 0.1713 - val_acc: 0.9323\n",
      "Epoch 3/10\n",
      "563/562 [==============================] - 734s - loss: 0.1794 - acc: 0.9289 - val_loss: 0.1769 - val_acc: 0.9273\n",
      "Epoch 4/10\n",
      "563/562 [==============================] - 733s - loss: 0.1769 - acc: 0.9300 - val_loss: 0.1748 - val_acc: 0.9315\n",
      "Epoch 5/10\n",
      "563/562 [==============================] - 734s - loss: 0.1754 - acc: 0.9310 - val_loss: 0.1665 - val_acc: 0.9333\n",
      "Epoch 6/10\n",
      "563/562 [==============================] - 737s - loss: 0.1736 - acc: 0.9307 - val_loss: 0.1655 - val_acc: 0.9340\n",
      "Epoch 7/10\n",
      "563/562 [==============================] - 739s - loss: 0.1723 - acc: 0.9316 - val_loss: 0.1710 - val_acc: 0.9298\n",
      "Epoch 8/10\n",
      "563/562 [==============================] - 739s - loss: 0.1703 - acc: 0.9322 - val_loss: 0.1695 - val_acc: 0.9309\n",
      "Epoch 9/10\n",
      "563/562 [==============================] - 740s - loss: 0.1688 - acc: 0.9336 - val_loss: 0.1741 - val_acc: 0.9312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0ed648fb00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#continue with reduced learning rate\n",
    "model1.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(lr=0.0005),\n",
    "              metrics=['accuracy']) \n",
    "model1.fit_generator(datagen.flow(x_train,y_train_2, batch_size = 64), validation_data=(x_val, y_val_2),\n",
    "                   epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b40d1c730857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.845798008543\n",
      "thresh: 0.1 \tF2 score: 0.862875233777\n",
      "thresh: 0.15 \tF2 score: 0.86693817779\n",
      "thresh: 0.2 \tF2 score: 0.869761844756\n",
      "thresh: 0.25 \tF2 score: 0.866331314693\n",
      "thresh: 0.3 \tF2 score: 0.862697797486\n",
      "thresh: 0.35 \tF2 score: 0.859893670385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model1.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_2, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "563/562 [==============================] - 739s - loss: 0.1609 - acc: 0.9364 - val_loss: 0.1601 - val_acc: 0.9359\n",
      "Epoch 2/10\n",
      "563/562 [==============================] - 733s - loss: 0.1594 - acc: 0.9367 - val_loss: 0.1568 - val_acc: 0.9374\n",
      "Epoch 3/10\n",
      "563/562 [==============================] - 732s - loss: 0.1586 - acc: 0.9375 - val_loss: 0.1563 - val_acc: 0.9364\n",
      "Epoch 4/10\n",
      "563/562 [==============================] - 733s - loss: 0.1579 - acc: 0.9375 - val_loss: 0.1563 - val_acc: 0.9378\n",
      "Epoch 5/10\n",
      "172/562 [========>.....................] - ETA: 487s - loss: 0.1588 - acc: 0.9382"
     ]
    }
   ],
   "source": [
    "#continue with reduced learning rate\n",
    "model1.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(lr=0.0002),\n",
    "              metrics=['accuracy']) \n",
    "model1.fit_generator(datagen.flow(x_train,y_train_2, batch_size = 64), validation_data=(x_val, y_val_2),\n",
    "                   epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  7/562 [..............................] - ETA: 728s - loss: 0.1564 - acc: 0.9340"
     ]
    }
   ],
   "source": [
    "model1.fit_generator(datagen.flow(x_train,y_train_2, batch_size = 64), validation_data=(x_val, y_val_2),\n",
    "                   epochs=5, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model1.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_2, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#continue with reduced learning rate\n",
    "model1.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(lr=0.0001),\n",
    "              metrics=['accuracy']) \n",
    "model1.fit_generator(datagen.flow(x_train,y_train_2, batch_size = 64), validation_data=(x_val, y_val_2),\n",
    "                  epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model1.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_2, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#continue with reduced learning rate\n",
    "model1.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(lr=0.00005),\n",
    "              metrics=['accuracy']) \n",
    "model1.fit_generator(datagen.flow(x_train,y_train_2, batch_size = 64), validation_data=(x_val, y_val_2),\n",
    "                   epochs=10, steps_per_epoch=x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model1.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_2, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model1.predict(x_val,batch_size=128)\n",
    "bestthresh = 0\n",
    "bestF2score = 0\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    F2score = fbeta_score(y_val_2, np.array(y_pred)>thresh, beta=2, average='samples')\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",F2score)\n",
    "    if F2score > bestF2score:\n",
    "        bestthresh = thresh\n",
    "        bestF2score = F2score        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total,tp,tn,fp,fn = multilabelmetrics(y_val_2,np.array(y_pred)>bestthresh)\n",
    "d = {'Total':total,'TP':tp,'TN':tn,'FP':fp,'FN':fn}\n",
    "pd.DataFrame(d, index=['others']+labels[7:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_rare = np.sum(np.sum(y_train[:,:7],axis=1)>0)\n",
    "print(num_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.save(\"simple_200_major\")\n",
    "model1.save_weights(\"simple_200_major_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40479/40479 [00:56<00:00, 715.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2180, 200, 200, 3)\n",
      "(2180, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_rare = 2180\n",
    "x_val = []\n",
    "x_train = []\n",
    "x_train = np.zeros((num_rare,200,200,3), np.float32)\n",
    "y_train = []\n",
    "\n",
    "df_train = pd.read_csv('train_v2.csv')\n",
    "\n",
    "labels = ['blow_down',\n",
    " 'bare_ground',\n",
    " 'conventional_mine',\n",
    " 'blooming',\n",
    " 'artisinal_mine',\n",
    " 'selective_logging',         \n",
    " 'slash_burn', \n",
    " 'cultivation',\n",
    " 'habitation',\n",
    " 'road',\n",
    " 'agriculture',\n",
    " 'water',\n",
    " 'primary',\n",
    " 'partly_cloudy', \n",
    " 'cloudy',\n",
    " 'clear',\n",
    " 'haze',]\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "i=0\n",
    "\n",
    "for f, tags in tqdm(df_train.values[:40479], miniters=1000):    \n",
    "    img = cv2.imread('train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    if(np.sum(targets[:7])>0):\n",
    "        x_train[i,:,:,:] = np.array(cv2.resize(img, (200, 200)),np.float32)/255.#139 minimum size for inception\n",
    "        i+=1\n",
    "        y_train.append(targets)\n",
    "    \n",
    "y_train = np.array(y_train, np.uint8)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1962, 200, 200, 3)\n",
      "(1962, 7)\n",
      "(218, 200, 200, 3)\n",
      "(218, 7)\n"
     ]
    }
   ],
   "source": [
    "train_mean = np.load('simple_200_train_mean.npy')\n",
    "x_train -= train_mean\n",
    "x_train, x_val, y_train_3, y_val_3 = train_test_split(x_train,y_train[:,:7],test_size=0.1)\n",
    "print(x_train.shape)\n",
    "print(y_train_3.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model for the rarer classes\n",
    "from keras.models import load_model\n",
    "model2 = load_model(\"simple_200_major\")\n",
    "for layer in model2.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "model2.layers[-1].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "307/306 [==============================] - 128s - loss: 0.4260 - acc: 0.8563 - val_loss: 0.2420 - val_acc: 0.9017\n",
      "Epoch 2/10\n",
      "307/306 [==============================] - 125s - loss: 0.2349 - acc: 0.9042 - val_loss: 0.2249 - val_acc: 0.9122\n",
      "Epoch 3/10\n",
      "307/306 [==============================] - 125s - loss: 0.2178 - acc: 0.9095 - val_loss: 0.2143 - val_acc: 0.9128\n",
      "Epoch 4/10\n",
      "307/306 [==============================] - 125s - loss: 0.2104 - acc: 0.9126 - val_loss: 0.2122 - val_acc: 0.9135\n",
      "Epoch 5/10\n",
      "307/306 [==============================] - 125s - loss: 0.2057 - acc: 0.9151 - val_loss: 0.2076 - val_acc: 0.9135\n",
      "Epoch 6/10\n",
      "307/306 [==============================] - 125s - loss: 0.2027 - acc: 0.9161 - val_loss: 0.2059 - val_acc: 0.9201\n",
      "Epoch 7/10\n",
      "307/306 [==============================] - 125s - loss: 0.2000 - acc: 0.9168 - val_loss: 0.2040 - val_acc: 0.9174\n",
      "Epoch 8/10\n",
      "307/306 [==============================] - 126s - loss: 0.1983 - acc: 0.9175 - val_loss: 0.1985 - val_acc: 0.9220\n",
      "Epoch 9/10\n",
      "307/306 [==============================] - 125s - loss: 0.1965 - acc: 0.9178 - val_loss: 0.2021 - val_acc: 0.9187\n",
      "Epoch 10/10\n",
      "307/306 [==============================] - 126s - loss: 0.1946 - acc: 0.9184 - val_loss: 0.1981 - val_acc: 0.9207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f250023e470>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit_generator(datagen.flow(x_train,y_train_3, batch_size = 64), validation_data=(x_val, y_val_3),\n",
    "                  epochs=10, steps_per_epoch=10*x_train.shape[0]/ 64, callbacks=callbacks,\n",
    "                  )#more steps per epoch to compensate for fewer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.748415511604\n",
      "thresh: 0.1 \tF2 score: 0.783445530005\n",
      "thresh: 0.15 \tF2 score: 0.780312959212\n",
      "thresh: 0.2 \tF2 score: 0.77209264996\n",
      "thresh: 0.25 \tF2 score: 0.76379787389\n",
      "thresh: 0.3 \tF2 score: 0.732379496141\n",
      "thresh: 0.35 \tF2 score: 0.703109072375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_3, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "154/153 [==============================] - 124s - loss: 0.1870 - acc: 0.9216 - val_loss: 0.1980 - val_acc: 0.9194\n",
      "Epoch 2/10\n",
      "154/153 [==============================] - 120s - loss: 0.1865 - acc: 0.9221 - val_loss: 0.1975 - val_acc: 0.9187\n",
      "Epoch 3/10\n",
      "154/153 [==============================] - 118s - loss: 0.1849 - acc: 0.9229 - val_loss: 0.1970 - val_acc: 0.9174\n",
      "Epoch 4/10\n",
      "154/153 [==============================] - 119s - loss: 0.1849 - acc: 0.9223 - val_loss: 0.1990 - val_acc: 0.9174\n",
      "Epoch 5/10\n",
      "154/153 [==============================] - 117s - loss: 0.1850 - acc: 0.9223 - val_loss: 0.1992 - val_acc: 0.9155\n",
      "Epoch 6/10\n",
      "154/153 [==============================] - 120s - loss: 0.1850 - acc: 0.9216 - val_loss: 0.1963 - val_acc: 0.9194\n",
      "Epoch 7/10\n",
      "154/153 [==============================] - 120s - loss: 0.1838 - acc: 0.9232 - val_loss: 0.1958 - val_acc: 0.9194\n",
      "Epoch 8/10\n",
      "154/153 [==============================] - 121s - loss: 0.1834 - acc: 0.9233 - val_loss: 0.1983 - val_acc: 0.9201\n",
      "Epoch 9/10\n",
      "154/153 [==============================] - 121s - loss: 0.1833 - acc: 0.9227 - val_loss: 0.1960 - val_acc: 0.9187\n",
      "Epoch 10/10\n",
      "154/153 [==============================] - 121s - loss: 0.1829 - acc: 0.9232 - val_loss: 0.1963 - val_acc: 0.9194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24ff23e128>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(lr=0.0005),\n",
    "              metrics=['accuracy']) \n",
    "model2.fit_generator(datagen.flow(x_train,y_train_3, batch_size = 128), validation_data=(x_val, y_val_3),\n",
    "                  epochs=10, steps_per_epoch=10*x_train.shape[0]/ 128, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.763513245165\n",
      "thresh: 0.1 \tF2 score: 0.792408852615\n",
      "thresh: 0.15 \tF2 score: 0.786238035665\n",
      "thresh: 0.2 \tF2 score: 0.771118789467\n",
      "thresh: 0.25 \tF2 score: 0.757592305757\n",
      "thresh: 0.3 \tF2 score: 0.755315275957\n",
      "thresh: 0.35 \tF2 score: 0.714831804281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_3, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "154/153 [==============================] - 123s - loss: 0.1815 - acc: 0.9231 - val_loss: 0.1962 - val_acc: 0.9201\n",
      "Epoch 2/10\n",
      "154/153 [==============================] - 121s - loss: 0.1802 - acc: 0.9245 - val_loss: 0.1967 - val_acc: 0.9194\n",
      "Epoch 3/10\n",
      "154/153 [==============================] - 122s - loss: 0.1815 - acc: 0.9235 - val_loss: 0.1964 - val_acc: 0.9194\n",
      "Epoch 4/10\n",
      "154/153 [==============================] - 121s - loss: 0.1824 - acc: 0.9228 - val_loss: 0.1959 - val_acc: 0.9187\n",
      "Epoch 5/10\n",
      "154/153 [==============================] - 120s - loss: 0.1814 - acc: 0.9233 - val_loss: 0.1960 - val_acc: 0.9187\n",
      "Epoch 6/10\n",
      "154/153 [==============================] - 122s - loss: 0.1805 - acc: 0.9238 - val_loss: 0.1961 - val_acc: 0.9207\n",
      "Epoch 7/10\n",
      "154/153 [==============================] - 122s - loss: 0.1819 - acc: 0.9227 - val_loss: 0.1960 - val_acc: 0.9194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24ff07c9e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(lr=0.0002),\n",
    "              metrics=['accuracy']) \n",
    "model2.fit_generator(datagen.flow(x_train,y_train_3, batch_size = 128), validation_data=(x_val, y_val_3),\n",
    "                  epochs=10, steps_per_epoch=10*x_train.shape[0]/ 128, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.764268669659\n",
      "thresh: 0.1 \tF2 score: 0.79180815362\n",
      "thresh: 0.15 \tF2 score: 0.785364291672\n",
      "thresh: 0.2 \tF2 score: 0.773958457445\n",
      "thresh: 0.25 \tF2 score: 0.759339793743\n",
      "thresh: 0.3 \tF2 score: 0.748216106014\n",
      "thresh: 0.35 \tF2 score: 0.714831804281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_3, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "154/153 [==============================] - 124s - loss: 0.1799 - acc: 0.9246 - val_loss: 0.1968 - val_acc: 0.9194\n",
      "Epoch 2/10\n",
      "154/153 [==============================] - 120s - loss: 0.1804 - acc: 0.9238 - val_loss: 0.1969 - val_acc: 0.9194\n",
      "Epoch 3/10\n",
      "154/153 [==============================] - 122s - loss: 0.1800 - acc: 0.9244 - val_loss: 0.1966 - val_acc: 0.9194\n",
      "Epoch 4/10\n",
      "154/153 [==============================] - 121s - loss: 0.1810 - acc: 0.9240 - val_loss: 0.1963 - val_acc: 0.9194\n",
      "Epoch 5/10\n",
      "154/153 [==============================] - 121s - loss: 0.1805 - acc: 0.9241 - val_loss: 0.1962 - val_acc: 0.9187\n",
      "Epoch 6/10\n",
      "154/153 [==============================] - 122s - loss: 0.1798 - acc: 0.9235 - val_loss: 0.1965 - val_acc: 0.9187\n",
      "Epoch 7/10\n",
      "154/153 [==============================] - 121s - loss: 0.1791 - acc: 0.9247 - val_loss: 0.1958 - val_acc: 0.9187\n",
      "Epoch 8/10\n",
      "154/153 [==============================] - 122s - loss: 0.1804 - acc: 0.9240 - val_loss: 0.1959 - val_acc: 0.9194\n",
      "Epoch 9/10\n",
      "154/153 [==============================] - 121s - loss: 0.1798 - acc: 0.9234 - val_loss: 0.1965 - val_acc: 0.9187\n",
      "Epoch 10/10\n",
      "154/153 [==============================] - 117s - loss: 0.1797 - acc: 0.9239 - val_loss: 0.1963 - val_acc: 0.9187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24fee472e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(lr=0.0001),\n",
    "              metrics=['accuracy']) \n",
    "model2.fit_generator(datagen.flow(x_train,y_train_3, batch_size = 128), validation_data=(x_val, y_val_3),\n",
    "                  epochs=10, steps_per_epoch=10*x_train.shape[0]/ 128, callbacks=callbacks,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.762530283172\n",
      "thresh: 0.1 \tF2 score: 0.784772694176\n",
      "thresh: 0.15 \tF2 score: 0.788367786648\n",
      "thresh: 0.2 \tF2 score: 0.772081893642\n",
      "thresh: 0.25 \tF2 score: 0.760322755736\n",
      "thresh: 0.3 \tF2 score: 0.752038735984\n",
      "thresh: 0.35 \tF2 score: 0.717889908257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_val,batch_size=128)\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",fbeta_score(y_val_3, np.array(y_pred)>thresh, beta=2, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresh: 0.05 \tF2 score: 0.763941015661\n",
      "thresh: 0.1 \tF2 score: 0.78436312668\n",
      "thresh: 0.15 \tF2 score: 0.7818420112\n",
      "thresh: 0.2 \tF2 score: 0.76978831566\n",
      "thresh: 0.25 \tF2 score: 0.760104319737\n",
      "thresh: 0.3 \tF2 score: 0.745158002039\n",
      "thresh: 0.35 \tF2 score: 0.717125382263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cs231n/myVE35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model2.predict(x_val,batch_size=128)\n",
    "bestthresh = 0\n",
    "bestF2score = 0\n",
    "for thresh in [0.05,0.1,0.15,0.2,0.25,0.3,0.35]:\n",
    "    F2score = fbeta_score(y_val_3, np.array(y_pred)>thresh, beta=2, average='samples')\n",
    "    print(\"thresh:\",thresh,\"\\tF2 score:\",F2score)\n",
    "    if F2score > bestF2score:\n",
    "        bestthresh = thresh\n",
    "        bestF2score = F2score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blow_down</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>194</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bare_ground</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>58</td>\n",
       "      <td>86</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conventional_mine</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>191</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blooming</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>161</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artisinal_mine</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>151</td>\n",
       "      <td>38</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selective_logging</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>123</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slash_burn</th>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>168</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   FN  FP   TN  TP  Total\n",
       "blow_down           5  16  194   3      8\n",
       "bare_ground         4  70   58  86     90\n",
       "conventional_mine   1  20  191   6      7\n",
       "blooming            0  29  161  28     28\n",
       "artisinal_mine      2  27  151  38     40\n",
       "selective_logging   1  60  123  34     35\n",
       "slash_burn          2  31  168  17     19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total,tp,tn,fp,fn = multilabelmetrics(y_val_3,np.array(y_pred)>bestthresh)\n",
    "d = {'Total':total,'TP':tp,'TN':tn,'FP':fp,'FN':fn}\n",
    "pd.DataFrame(d, index=labels[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2.save(\"simple_200_rare\")\n",
    "model2.save_weights(\"simple_200_rare_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40479/40479 [01:16<00:00, 532.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479, 200, 200, 3)\n",
      "(40479, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#F2 score on training set\n",
    "x_val = []\n",
    "x_train = []\n",
    "x_train = np.zeros((40479,200,200,3), np.float32)\n",
    "y_train = []\n",
    "\n",
    "df_train = pd.read_csv('train_v2.csv')\n",
    "\n",
    "labels = ['blow_down',\n",
    " 'bare_ground',\n",
    " 'conventional_mine',\n",
    " 'blooming',\n",
    " 'artisinal_mine',\n",
    " 'selective_logging',         \n",
    " 'slash_burn', \n",
    " 'cultivation',\n",
    " 'habitation',\n",
    " 'road',\n",
    " 'agriculture',\n",
    " 'water',\n",
    " 'primary',\n",
    " 'partly_cloudy', \n",
    " 'cloudy',\n",
    " 'clear',\n",
    " 'haze',]\n",
    "\n",
    "label_map = {l: i for i, l in enumerate(labels)}\n",
    "inv_label_map = {i: l for l, i in label_map.items()}\n",
    "\n",
    "i=0\n",
    "\n",
    "for f, tags in tqdm(df_train.values[:40479], miniters=1000):    \n",
    "    img = cv2.imread('train-jpg/{}.jpg'.format(f))\n",
    "    targets = np.zeros(17)\n",
    "    for t in tags.split(' '):\n",
    "        targets[label_map[t]] = 1 \n",
    "    x_train[i,:,:,:] = np.array(cv2.resize(img, (200, 200)),np.float32)/255.#139 minimum size for inception\n",
    "    i+=1\n",
    "    y_train.append(targets)\n",
    "\n",
    "y_train = np.array(y_train, np.uint8)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train -= train_mean\n",
    "model = Sequential()#using same architecture for all three models\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', input_shape=(200, 200, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2048))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1024))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.load_weights('simple_200_weather_weights')\n",
    "model1 = Sequential()\n",
    "model1.add(Conv2D(32, (3, 3), padding = 'same', input_shape=(200,200,3)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(48, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Conv2D(128, (3, 3), padding = 'same'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(2048))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(1024))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dense(7, activation='sigmoid'))\n",
    "model1.load_weights('simple_200_major_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = model.predict(x_train,batch_size=128)\n",
    "y1 = model1.predict(x_train,batch_size=128)\n",
    "y2 = model2.predict(x_train,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40479, 4)\n",
      "(40479, 7)\n",
      "(40479, 7)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y1.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_predictions_2(y,y1,y2,thresh):\n",
    "    y_pred = np.zeros((y.shape[0],17),np.uint8)\n",
    "    y_bool = np.array((y > thresh[0:4]),np.uint8)\n",
    "    y1_bool = np.array((y1[:,1:] > thresh[4:10]),np.uint8)\n",
    "    y2_bool = np.array((y2 > thresh[10:17])*np.tile(y1[:,0]>thresh[17],(7,1)).T,np.uint8)\n",
    "    y_pred[:,:7] = y2_bool\n",
    "    y_pred[:,7:13] = y1_bool\n",
    "    y_pred[:,13:] = y_bool\n",
    "    y_pred[((np.nonzero(y[:,1]>thresh[18]))[0]),:] = 0\n",
    "    y_pred[((np.nonzero(y[:,1]>thresh[18]))[0]),14] = 1\n",
    "    return y_pred\n",
    "\n",
    "def optimise_f2_thresholds(x_init,y, y1,y2, ytrue, num_thresh=19, verbose=True, resolution=100):\n",
    "  def mf(x):\n",
    "    p2 = combine_predictions_2(y, y1,y2,x)\n",
    "    score = fbeta_score(ytrue, p2, beta=2, average='samples')\n",
    "    return score\n",
    "\n",
    "  x = list(x_init)\n",
    "  for i in range(num_thresh):\n",
    "    best_i2 = 0\n",
    "    best_score = 0\n",
    "    for i2 in range(resolution):\n",
    "      i2 /= resolution\n",
    "      x[i] = i2\n",
    "      score = mf(x)\n",
    "      if score > best_score:\n",
    "        best_i2 = i2\n",
    "        best_score = score\n",
    "    x[i] = best_i2\n",
    "    if verbose:\n",
    "      print(i, best_i2, best_score)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21 0.918700015484\n",
      "1 0.09 0.918923133532\n",
      "2 0.22 0.918955179887\n",
      "3 0.12 0.919173172716\n",
      "4 0.18 0.919250278888\n",
      "5 0.24 0.919299402384\n",
      "6 0.23 0.919335736141\n",
      "7 0.19 0.919354111783\n",
      "8 0.21 0.919373788484\n",
      "9 0.14 0.919382161617\n",
      "10 0.33 0.919425953016\n",
      "11 0.3 0.91949832561\n",
      "12 0.21 0.919505783726\n",
      "13 0.6 0.919595727831\n",
      "14 0.32 0.919617563736\n",
      "15 0.52 0.919963089155\n",
      "16 0.69 0.920055842168\n",
      "17 0.23 0.920115614524\n",
      "18 0.75 0.930827678634\n"
     ]
    }
   ],
   "source": [
    "best_thresh = optimise_f2_thresholds([0.2]*19,y,y1,y2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21 0.930827678634\n",
      "1 0.09 0.930827678634\n",
      "2 0.22 0.930827678634\n",
      "3 0.12 0.930827678634\n",
      "4 0.18 0.930827678634\n",
      "5 0.24 0.930827678634\n",
      "6 0.23 0.930827678634\n",
      "7 0.21 0.930835409936\n",
      "8 0.23 0.930838946472\n",
      "9 0.22 0.930913670254\n",
      "10 0.71 0.930914214303\n",
      "11 0.3 0.930914214303\n",
      "12 0.21 0.930914214303\n",
      "13 0.35 0.930933458898\n",
      "14 0.32 0.930933458898\n",
      "15 0.51 0.93093402694\n",
      "16 0.73 0.930936379718\n",
      "17 0.23 0.930936379718\n",
      "18 0.75 0.930936379718\n"
     ]
    }
   ],
   "source": [
    "best_thresh = optimise_f2_thresholds(best_thresh,y,y1,y2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.21 0.930936379718\n",
      "1 0.09 0.930936379718\n",
      "2 0.22 0.930936379718\n",
      "3 0.12 0.930936379718\n",
      "4 0.18 0.930936379718\n",
      "5 0.24 0.930936379718\n",
      "6 0.23 0.930936379718\n",
      "7 0.21 0.930936379718\n",
      "8 0.23 0.930936379718\n",
      "9 0.22 0.930936379718\n",
      "10 0.71 0.930936379718\n",
      "11 0.3 0.930936379718\n",
      "12 0.21 0.930936379718\n",
      "13 0.35 0.930936379718\n",
      "14 0.32 0.930936379718\n",
      "15 0.51 0.930936379718\n",
      "16 0.73 0.930936379718\n",
      "17 0.23 0.930936379718\n",
      "18 0.75 0.930936379718\n"
     ]
    }
   ],
   "source": [
    "best_thresh = optimise_f2_thresholds(best_thresh,y,y1,y2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21, 0.09, 0.22, 0.12, 0.18, 0.24, 0.23, 0.21, 0.23, 0.22, 0.71, 0.3, 0.21, 0.35, 0.32, 0.51, 0.73, 0.23, 0.75]\n"
     ]
    }
   ],
   "source": [
    "print(best_thresh)\n",
    "np.save('3net_200_best_thresh',best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TP</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blow_down</th>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>40372</td>\n",
       "      <td>14</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bare_ground</th>\n",
       "      <td>445</td>\n",
       "      <td>722</td>\n",
       "      <td>38895</td>\n",
       "      <td>417</td>\n",
       "      <td>862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conventional_mine</th>\n",
       "      <td>26</td>\n",
       "      <td>105</td>\n",
       "      <td>40274</td>\n",
       "      <td>74</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blooming</th>\n",
       "      <td>220</td>\n",
       "      <td>255</td>\n",
       "      <td>39892</td>\n",
       "      <td>112</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artisinal_mine</th>\n",
       "      <td>59</td>\n",
       "      <td>147</td>\n",
       "      <td>39993</td>\n",
       "      <td>280</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>selective_logging</th>\n",
       "      <td>196</td>\n",
       "      <td>205</td>\n",
       "      <td>39934</td>\n",
       "      <td>144</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slash_burn</th>\n",
       "      <td>205</td>\n",
       "      <td>2</td>\n",
       "      <td>40268</td>\n",
       "      <td>4</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cultivation</th>\n",
       "      <td>976</td>\n",
       "      <td>3662</td>\n",
       "      <td>32340</td>\n",
       "      <td>3501</td>\n",
       "      <td>4477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>habitation</th>\n",
       "      <td>568</td>\n",
       "      <td>1652</td>\n",
       "      <td>35167</td>\n",
       "      <td>3092</td>\n",
       "      <td>3660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>road</th>\n",
       "      <td>602</td>\n",
       "      <td>2669</td>\n",
       "      <td>29740</td>\n",
       "      <td>7468</td>\n",
       "      <td>8070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agriculture</th>\n",
       "      <td>628</td>\n",
       "      <td>3846</td>\n",
       "      <td>24318</td>\n",
       "      <td>11687</td>\n",
       "      <td>12315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>1119</td>\n",
       "      <td>2254</td>\n",
       "      <td>30814</td>\n",
       "      <td>6292</td>\n",
       "      <td>7411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>primary</th>\n",
       "      <td>171</td>\n",
       "      <td>1143</td>\n",
       "      <td>1823</td>\n",
       "      <td>37342</td>\n",
       "      <td>37513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partly_cloudy</th>\n",
       "      <td>173</td>\n",
       "      <td>1250</td>\n",
       "      <td>31968</td>\n",
       "      <td>7088</td>\n",
       "      <td>7261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cloudy</th>\n",
       "      <td>42</td>\n",
       "      <td>1325</td>\n",
       "      <td>37065</td>\n",
       "      <td>2047</td>\n",
       "      <td>2089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clear</th>\n",
       "      <td>244</td>\n",
       "      <td>2144</td>\n",
       "      <td>9904</td>\n",
       "      <td>28187</td>\n",
       "      <td>28431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haze</th>\n",
       "      <td>384</td>\n",
       "      <td>1891</td>\n",
       "      <td>35891</td>\n",
       "      <td>2313</td>\n",
       "      <td>2697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     FN    FP     TN     TP  Total\n",
       "blow_down            84     9  40372     14     98\n",
       "bare_ground         445   722  38895    417    862\n",
       "conventional_mine    26   105  40274     74    100\n",
       "blooming            220   255  39892    112    332\n",
       "artisinal_mine       59   147  39993    280    339\n",
       "selective_logging   196   205  39934    144    340\n",
       "slash_burn          205     2  40268      4    209\n",
       "cultivation         976  3662  32340   3501   4477\n",
       "habitation          568  1652  35167   3092   3660\n",
       "road                602  2669  29740   7468   8070\n",
       "agriculture         628  3846  24318  11687  12315\n",
       "water              1119  2254  30814   6292   7411\n",
       "primary             171  1143   1823  37342  37513\n",
       "partly_cloudy       173  1250  31968   7088   7261\n",
       "cloudy               42  1325  37065   2047   2089\n",
       "clear               244  2144   9904  28187  28431\n",
       "haze                384  1891  35891   2313   2697"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total,tp,tn,fp,fn = multilabelmetrics(y_train,combine_predictions_2(y,y1,y2,best_thresh))\n",
    "d = {'Total':total,'TP':tp,'TN':tn,'FP':fp,'FN':fn}\n",
    "pd.DataFrame(d, index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [05:47<00:00, 86.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 200, 200, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-7f39fe57ec79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m31191\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test set\n",
    "x_train = []\n",
    "x_val = []\n",
    "x_test = np.zeros((30000,200,200,3), np.float32)\n",
    "y_train = []\n",
    "\n",
    "df_test = pd.read_csv('sample_submission_v2.csv')\n",
    "\n",
    "i = 0 \n",
    "for f, tags in tqdm(df_test.values[:30000], miniters=1000):\n",
    "    img = cv2.imread('test-jpg/{}.jpg'.format(f))\n",
    "    x_test[i,:,:,:] = np.array(cv2.resize(img, (200, 200)),np.float32)/255.#139 minimum size for inception\n",
    "    i+=1\n",
    "print(x_test.shape)\n",
    "\n",
    "x_test -= train_mean\n",
    "\n",
    "y = model.predict(x_test,batch_size=128)\n",
    "y1 = model1.predict(x_test,batch_size=128)\n",
    "y2 = model2.predict(x_test,batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31191/31191 [02:03<00:00, 253.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31191, 200, 200, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_test=[]\n",
    "x_test = np.zeros((31191,200,200,3), np.float32)\n",
    "y_train = []\n",
    "\n",
    "df_test = pd.read_csv('sample_submission_v2.csv')\n",
    "\n",
    "i = 0 \n",
    "for f, tags in tqdm(df_test.values[30000:], miniters=1000):\n",
    "    img = cv2.imread('test-jpg/{}.jpg'.format(f))\n",
    "    x_test[i,:,:,:] = np.array(cv2.resize(img, (200, 200)),np.float32)/255.#139 minimum size for inception\n",
    "    i+=1\n",
    "print(x_test.shape)\n",
    "\n",
    "x_test -= train_mean\n",
    "\n",
    "y_1 = model.predict(x_test,batch_size=128)\n",
    "y1_1 = model1.predict(x_test,batch_size=128)\n",
    "y2_1 = model2.predict(x_test,batch_size=128)\n",
    "\n",
    "y = np.concatenate([y,y_1])\n",
    "y1 = np.concatenate([y1,y1_1])\n",
    "y2 = np.concatenate([y2,y2_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61191, 4)\n",
      "(61191, 7)\n",
      "(61191, 7)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y1.shape)\n",
    "print(y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61191, 17)\n"
     ]
    }
   ],
   "source": [
    "y_pred = combine_predictions_2(y,y1,y2,best_thresh)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_np = np.array(labels)\n",
    "preds = [' '.join(labels_np[np.array(y_pred[i,:],bool)]) for i in range(y_pred.shape[0])]\n",
    "subm = pd.DataFrame()\n",
    "subm['image_name'] = df_test.values[:,0]\n",
    "subm['tags'] = preds\n",
    "subm.to_csv('submission_200_3net_1.csv', index=False)\n",
    "#test set score:0.92387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61191, 17)\n"
     ]
    }
   ],
   "source": [
    "thresh,thresh1,thresh2,thresh3,thresh4 = f2scorelistfiner[1][:5]\n",
    "y_pred = combine_predictions_2(x_test,y,y1,y2,thresh,thresh1,thresh2,thresh3,thresh4)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_np = np.array(labels)\n",
    "preds = [' '.join(labels_np[np.array(y_pred[i,:],bool)]) for i in range(y_pred.shape[0])]\n",
    "subm = pd.DataFrame()\n",
    "subm['image_name'] = df_test.values[:,0]\n",
    "subm['tags'] = preds\n",
    "subm.to_csv('submission_96_3net_2.csv', index=False)\n",
    "#test set score:0.92445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61191, 17)\n"
     ]
    }
   ],
   "source": [
    "thresh,thresh1,thresh2,thresh3,thresh4 = f2scorelistfiner[2][:5]\n",
    "y_pred = combine_predictions_2(x_test,y,y1,y2,thresh,thresh1,thresh2,thresh3,thresh4)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_np = np.array(labels)\n",
    "preds = [' '.join(labels_np[np.array(y_pred[i,:],bool)]) for i in range(y_pred.shape[0])]\n",
    "subm = pd.DataFrame()\n",
    "subm['image_name'] = df_test.values[:,0]\n",
    "subm['tags'] = preds\n",
    "subm.to_csv('submission_96_3net_3.csv', index=False)\n",
    "#test set score:0.92446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61191, 17)\n"
     ]
    }
   ],
   "source": [
    "thresh,thresh1,thresh2,thresh3,thresh4 = f2scorelistfiner[3][:5]\n",
    "y_pred = combine_predictions_2(x_test,y,y1,y2,thresh,thresh1,thresh2,thresh3,thresh4)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_np = np.array(labels)\n",
    "preds = [' '.join(labels_np[np.array(y_pred[i,:],bool)]) for i in range(y_pred.shape[0])]\n",
    "subm = pd.DataFrame()\n",
    "subm['image_name'] = df_test.values[:,0]\n",
    "subm['tags'] = preds\n",
    "subm.to_csv('submission_96_3net_4.csv', index=False)\n",
    "#test set score: 0.92457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61191, 17)\n"
     ]
    }
   ],
   "source": [
    "thresh,thresh1,thresh2,thresh3,thresh4 = f2scorelistfiner[4][:5]\n",
    "y_pred = combine_predictions_2(x_test,y,y1,y2,thresh,thresh1,thresh2,thresh3,thresh4)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_np = np.array(labels)\n",
    "preds = [' '.join(labels_np[np.array(y_pred[i,:],bool)]) for i in range(y_pred.shape[0])]\n",
    "subm = pd.DataFrame()\n",
    "subm['image_name'] = df_test.values[:,0]\n",
    "subm['tags'] = preds\n",
    "subm.to_csv('submission_96_3net_5.csv', index=False)\n",
    "#test set score: 0.92455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
